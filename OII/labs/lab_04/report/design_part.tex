\chapter{Конструкторская часть}


\section{Функция активации ReLU}

Функция активации ReLU (Rectified Linear Unit) является одной из наиболее часто используемых функций в глубоких нейронных сетях. Она определяется следующим образом.

\begin{equation}
	f(x) = max(0, x),
\end{equation}
где x — входное значение нейрона.

Достоинства:
\begin{itemize}[label*=---]
	\item высокая вычислительная эффективность: благодаря своей линейной структуре на положительном промежутке, ReLU требует минимальных вычислительных ресурсов, что ускоряет обработку данных;
	\item ускоренное обучение: в отличие от сигмоидных функций, которые могут приводить к затуханию градиента, ReLU сохраняет ненулевые градиенты для положительных значений, что способствует более быстрой сходимости алгоритма;
	\item стимуляция разреженности: для отрицательных значений активация равна нулю, что уменьшает количество активных нейронов, помогая модели выявлять более компактные и информативные представления данных;
\end{itemize}

Недостатки:
\begin{itemize}[label*=---]
	\item эффект <<мёртвых нейронов>>: если значения x на входе постоянно отрицательны, нейроны перестают обновляться, что снижает эффективность обучения;
	\item чувствительность к большим градиентам: в некоторых случаях большие значения градиентов могут привести к нестабильности обучения;
\end{itemize}
Для исправления описанных недостатков были предложены модификации, такие как Leaky ReLU, которая сохраняет небольшую активацию для отрицательных значений x.



\section{Функция потерь Cross-Entropy}

Функция потерь Cross-Entropy (перекрёстная энтропия) применяется для оценки отклонения между истинным распределением вероятностей $P$ и предсказанным распределением $Q$. Её выражение записывается следующим образом.
\begin{equation}
	L = -\sum_{i} P(i)log(Q(i)),
\end{equation}
где $P(i)$ — истинная вероятность класса $i$, а $Q(i)$ — предсказанная вероятность для этого класса.

Достоинства:
\begin{itemize}[label*=---]
	\item устойчивость к несбалансированным данным: перекрёстная энтропия лучше подходит для тех задач классификации, где распределение классов может быть смещённым;
	\item прямая корреляция с качеством: функция измеряет, насколько близко предсказанное распределение $Q$ совпадает с истинным распределением $P$, поэтому может считаться метрикой качества;
\end{itemize}



\section{Расчёт минимального размера обучающей выборки}

Для гарантированного достижения заданной точности классификации можно использовать неравенство Чебышёва:
\label{neq}
\begin{equation}
	P(| X - \mu | \ge k\sigma) \le \frac{1}{k^2},
\end{equation}
где $\mu$ --- математическое ожидание, $\sigma$ --- среднеквадратичное отклонение, $k$ --- коэффициент.

Согласно неравенству \ref{neq}, минимальный объём выборки $N$ можно оценить как
\begin{equation}
	N \ge \frac{1}{\epsilon^2\delta},
\end{equation}
где $\epsilon$ --- допустимая ошибка, а $\delta$ --- вероятность отклонения от математического ожидания. Эти расчёты позволяют определить нижнюю границу объёма данных, необходимых для успешного обучения модели.



\clearpage
