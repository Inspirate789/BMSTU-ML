\chapter{Формирование n-грамм с помощью цепей Маркова}

\section{Средства реализации}

В качестве языка программирования для реализации выбранных алгоритмов был выбран язык программирования Python \cite{pythonlang} ввиду наличия библиотек, реализующих цепи Маркова. Обучение модели для генерации текстов проводилось с использованием библиотеки markovify~\cite{markovify}.

\section{Реализация алгоритма}

На листинге \ref{lst:1} представлена реализация алгоритма обучения модели, генерации текстов на основе затравочного слова и подсчёта частоты генерации словосочетаний.

\begin{lstlisting}[label=lst:1,caption=Алгоритм генерации предложений на основе обучающей выборки и затравочного слова]
	 from collections import defaultdict
	 import markovify
	 
	 start_str = "@Доказательство@"
	 sentences_count = 1000
	 
	 with open("texts/@Кнут-том-1@.txt") as f:
		 text = f.read()
		 for size in [1, 2, 4]:
			 print(f'\nn = {size+1}:')
			 sentences = defaultdict(int)
			 text_model = markovify.Text(text, state_size=size)
			 for i in range(sentences_count):
				 sentence = text_model.make_sentence_with_start(start_str, strict=False, tries=100)
				 sentences[sentence] += 1
	 
		 print(f'generated {len(sentences)} different sentences')
		 
		 for sentence in sorted(sentences, key=sentences.get, reverse=True)[:5]:
		 	print(f'{sentences[sentence]/sentences_count:.3f}:\t {sentence}')
\end{lstlisting}

\section{Оценка <<человечности>> сгенерированных текстов}

Для оценки <<человечности>> генерируемых текстов был проведён опрос экспертов: Кириченко С. П. и Царева А. А.
В таблице \ref{tab:labels1} представлена экспертная оценка <<человечности>> текстов, сгенерированных на основе затравочного слова <<Доказательство>>. Заметим, что для n=5 на основе обучающей выборки сгенерировать тексты для выбранного затравочного слова не удалось. 

\begin{table}[H]
	\centering
	\caption{Экспертная оценка <<человечности>> текстов, сгенерированных на основе затравочного слова <<Доказательство>>}
	\label{tab:labels1}
	\renewcommand{\arraystretch}{1.7}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\multicolumn{1}{ |c|  }{\multirow{2}{*}{n}} & \multicolumn{1}{ |c|  }{\multirow{2}{*}{Предложение}} & \multicolumn{2}{ c| }{Экспертная оценка} \\ \cline{3-4}
		& & Кириченко С. П. & Царев А. А. \\ \hline
		\multirow{5}{*}{2} & \parbox{9.5cm}{\linespread{0.8}\selectfont Доказательство данного дерева.}&-&- \\ \cline{2-4}
		& \parbox{9.5cm}{\linespread{0.8}\selectfont Доказательство корректности алгоритма прекращается.}&+&+ \\ \cline{2-4}
		& \parbox{9.5cm}{\linespread{0.8}\selectfont Доказательство корректности алгоритма заканчивается.}&+&+ \\ \cline{2-4}
		& \parbox{9.5cm}{\linespread{0.8}\selectfont Доказательство данного корня.}&-&- \\ \cline{2-4}
		& \parbox{9.5cm}{\linespread{0.8}\selectfont Доказательство данного раздела.}&-&+ \\ \hline
		\multirow{5}{*}{3} & \parbox{9.5cm}{\linespread{0.8}\selectfont Доказательство проводим индукцией по n.}&+&+ \\ \cline{2-4}
		& \parbox{9.5cm}{\linespread{0.8}\selectfont Доказательство корректности алгоритма имеет и другой, еще более удачное решение, которое приведено ниже.}&-&+ \\ \cline{2-4}
		& \parbox{9.5cm}{\linespread{0.8}\selectfont Доказательство в таком порядке.}&-&- \\ \cline{2-4}
		& \parbox{9.5cm}{\linespread{0.8}\selectfont Доказательство того факта, что частота каждой конкретной даты пасхи для заданного года, пред- полагая, что номер этого года не превышает количества действий S при их считывании слева направо.}&-&- \\ \cline{2-4}
		& \parbox{9.5cm}{\linespread{0.8}\selectfont Доказательство того факта, что частота каждой конкретной даты пасхи для заданного года, пред- полагая, что номер этого года не превышает 100000.}&-&- \\ \hline
		5 & \parbox{9.5cm}{\linespread{0.8}\selectfont Доказательство проводим индукцией по m; при m = 1 и n = 2 выполнение алгоритма никогда не закончится.}&+&+ \\ \hline
	\end{tabular}
\end{table}

В таблице \ref{tab:labels2} представлена экспертная оценка <<человечности>> текстов, сгенерированных на основе затравочного слова <<Свойство>>. Заметим, что для n=5 на основе обучающей выборки сгенерировать тексты для выбранного затравочного слова не удалось. 

\begin{table}[H]
	\centering
	\caption{Экспертная оценка <<человечности>> текстов, сгенерированных на основе затравочного слова <<Свойство>>}
	\label{tab:labels2}
	\renewcommand{\arraystretch}{1.7}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\multicolumn{1}{ |c|  }{\multirow{2}{*}{n}} & \multicolumn{1}{ |c|  }{\multirow{2}{*}{Предложение}} & \multicolumn{2}{ c| }{Экспертная оценка} \\ \cline{3-4}
		& & Кириченко С. П. & Царев А. А. \\ \hline
		\multirow{5}{*}{2} & \parbox{9.5cm}{\linespread{0.8}\selectfont Свойство ортогональности перестановок, расположенных подряд единиц.}&-&- \\ \cline{2-4}
		& \parbox{9.5cm}{\linespread{0.8}\selectfont Свойство ортогональности перестановок, 199, 201, 211, Харари Ф.}&-&+ \\ \cline{2-4}
		& \parbox{9.5cm}{\linespread{0.8}\selectfont Свойство ортогональности перестановок, 199, 210.}&+&+ \\ \cline{2-4}
		& \parbox{9.5cm}{\linespread{0.8}\selectfont Свойство ортогональности перестановок, 199, 206, 215, 217.}&+&+ \\ \cline{2-4}
		& \parbox{9.5cm}{\linespread{0.8}\selectfont Свойство A обозначений.}&+&- \\ \hline
		\multirow{5}{*}{3} & \parbox{9.5cm}{\linespread{0.8}\selectfont Свойство A всегда справедливо для конечных ориентированных графов.}&+&+ \\ \cline{2-4}
		& \parbox{9.5cm}{\linespread{0.8}\selectfont Свойство A всегда справедливо для операций сложения и умножения для списков введена Д.}&-&- \\ \cline{2-4}
		& \parbox{9.5cm}{\linespread{0.8}\selectfont Свойство A всегда справедливо для операций сложения и умножения для списков любого типа будут практически одинаковы.}&-&- \\ \cline{2-4}
		& \parbox{9.5cm}{\linespread{0.8}\selectfont Свойство A всегда справедливо для конечных множеств, причем упомянул, что он соответствует показанному справа тетрадному ти- пу, в четырех различных списках.}&-&- \\ \cline{2-4}
		& \parbox{9.5cm}{\linespread{0.8}\selectfont Свойство A всегда справедливо для всех комплексных x.}&+&+ \\ \hline
	\end{tabular}
\end{table}

В таблице \ref{tab:labels3} представлена экспертная оценка <<человечности>> текстов, сгенерированных на основе затравочного слова <<программа>>. Заметим, что для n=5 на основе обучающей выборки сгенерировать тексты для выбранного затравочного слова не удалось. 

\begin{table}[H]
	\centering
	\caption{Экспертная оценка <<человечности>> текстов, сгенерированных на основе затравочного слова <<программа>>}
	\label{tab:labels3}
	\renewcommand{\arraystretch}{1.7}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\multicolumn{1}{ |c|  }{\multirow{2}{*}{n}} & \multicolumn{1}{ |c|  }{\multirow{2}{*}{Предложение}} & \multicolumn{2}{ c| }{Экспертная оценка} \\ \cline{3-4}
		& & Кириченко С. П. & Царев А. А. \\ \hline
		\multirow{5}{*}{2} & \parbox{9.5cm}{\linespread{0.8}\selectfont программа поиска, 453.}&+&+ \\ \cline{2-4}
		& \parbox{9.5cm}{\linespread{0.8}\selectfont программа работает программа.}&-&- \\ \cline{2-4}
		& \parbox{9.5cm}{\linespread{0.8}\selectfont программа вычисления, 180.}&+&+ \\ \cline{2-4}
		& \parbox{9.5cm}{\linespread{0.8}\selectfont программа была использована Й.}&-&- \\ \cline{2-4}
		& \parbox{9.5cm}{\linespread{0.8}\selectfont программа вычисления, ввод, 254.}&-&- \\ \hline
		\multirow{5}{*}{3} & \parbox{9.5cm}{\linespread{0.8}\selectfont программа имеет следующий вид.}&+&+ \\ \cline{2-4}
		& \parbox{9.5cm}{\linespread{0.8}\selectfont программа является неполной в нескольких словах блока из имеющихся в настоящий момент в системе.}&-&- \\ \cline{2-4}
		& \parbox{9.5cm}{\linespread{0.8}\selectfont программа M является искомым значением.}&-&- \\ \cline{2-4}
		& \parbox{9.5cm}{\linespread{0.8}\selectfont программа переходит к ячейке ENTER.}&+&- \\ \cline{2-4}
		& \parbox{9.5cm}{\linespread{0.8}\selectfont программа в процессе внедрения Scrum.}&-&- \\ \hline
	\end{tabular}
\end{table}

Средняя <<человечность>> сгенерированных текстов составила 0.5 для n=2, 0.33 для n=3 и 1 для n=5. Для выявления корреляции между n и <<человечностью>> текстов следует провести подобное исследование на большем массиве данных.

\section{Работа с нестрогим порядком слов}

Для оценки возможности генерации текстов при помощи цепей Маркова на основе обучающей выборки, содержащей предложения с разным базовым порядком слов была взята выборка из предложений <<кошка съела мышку>> и <<мышку съела кошка>>.

На листинге \ref{lst:2} представлены результаты генерации тестов на основе описанной обучающей выборки для затравочного слова <<кошка>>.

\begin{lstlisting}[label=lst:2,caption=Наиболее частые сгенерированные словосочетания и их частота]
	generated 2926 different sentences
	0.333:	 @кошка съела кошка@
	0.084:	 @кошка съела кошка съела мышку@
	0.083:	 @кошка съела кошка съела кошка@
	0.083:	 @кошка съела мышку съела кошка@
	0.083:	 @кошка съела мышку съела мышку@
	0.021:	 @кошка съела кошка съела мышку съела мышку@
	0.021:	 @кошка съела кошка съела кошка съела кошка@
	0.021:	 @кошка съела мышку съела мышку съела мышку@
	0.021:	 @кошка съела мышку съела кошка съела мышку@
	0.021:	 @кошка съела кошка съела мышку съела кошка@
\end{lstlisting}

Ни одно из сгенерированных словосочетаний эксперты не сочли <<человечным>>.



\section*{Вывод}

В данном разделе были описаны детали реализации алгоритма генерации тестов, дана экспертная оценка <<человечности>> сгенерированных текстов и продемонстрирована работа с нестрогим порядком слов. При работе с обучающей выборкой, имеющей нестрогий базовый порядок слов, модель на основе цепей Маркова <<путается>>, зацикливая словосочетания и полностью искажая их смысл, содержащийся в обучающей выборке. Для избежания описанной проблемы предлагается использовать обучающие выборки большего объёма, а также фиксировать базовый порядок слов в предложениях.

\clearpage
