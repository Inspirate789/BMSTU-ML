\ssr{ЗАКЛЮЧЕНИЕ}

В рамках лабораторной работы был изучен потенциал использования цепей Маркова для генерации текстов на естественном языке.  Все поставленные задачи были выполнены.

\begin{enumerate}[label*=\arabic*.]
	\item Сформированы n-граммы для генерации с помощью цепи Маркова на основе обучающего текста.
	\item Сформированы тексты с различными затравочными начальными словами.
	\item Дана экспертная оценка <<человечности>> сформированных текстов.
	\item Проведено исследование возможности генерации текста при наличии обучающей выборки, состоящей только из предложений <<кошка съела мышку>> и <<мышку съела кошка>>, оценена опасность работы с языками с нестрогим порядком слов (не обязательно SVO).
	\item Осуществлена генерация текстов с помощью инструмента из представленной методички. Оценён получаемый порядок слов в генерируемых предложениях для разной версии qwen2.5.
\end{enumerate}

При работе с обучающей выборкой, имеющей нестрогий базовый порядок слов, модель на основе цепей Маркова <<путается>>, зацикливая словосочетания, полностью искажая их смысл, содержащийся в обучающей выборке. Для избежания описанной проблемы предлагается использовать обучающие выборки большего объёма, а также фиксировать базовый порядок слов в предложениях.

При изучении текстов, сгененрированных предобученной моделью qwen2.5 было замечено, что  <<человечность>> текстов напрямую зависит от числа параметров модели. Также было выдвинуто предположение о том, что обучающая выборка для модели qwen имела строгий базовый порядок слов в предложениях, а именно SVO.
