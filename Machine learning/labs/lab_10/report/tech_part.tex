\chapter{Технологическая часть}

\section{Средства реализации}

В качестве языка программирования для реализации алгоритмов был выбран язык программирования Python ввиду наличия библиотек для обучения регрессионных моделей, таких как sklearn и numpy.

\section{Реализация алгоритмов}

На листинге \ref{lst:1} представлена реализация алгоритма кластеризации респондентов, принимавших участие в социологическом исследовании.

\begin{lstlisting}[label=lst:1,caption=Кластеризация респондентов социологического исследования]
	import numpy as np
	import pandas as pd
	import seaborn as sns
	import matplotlib.pyplot as plt
	import re
	
	import warnings
	warnings.filterwarnings('ignore')
	
	from google.colab import drive
	drive.mount('/content/drive')
	
	dataset = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/ml\_lab\_08/@ММО\_ЛР8\_Исходные\_данные@.xlsx')
	dataset = dataset.drop(['@Респондент@'], axis=1)
	dataset['@Сообщество@'] = dataset['@Сообщество@'].apply(lambda it: int(re.findall(r'\b\d+\b', it)[0]))
	class_names = list(set(dataset['@Ощущаемое.счастье@']))
	class_names.remove('@Неизвестно@')
	
	from operator import itemgetter
	group = {
		0: 0,
		1: 0,
		2: 0,
		3: 0,
		4: 1,
		5: 1,
		6: 1,
		7: 2,
		8: 2,
		9: 2,
	}
	rank_mapping = {
		'Prospering': 0,
		'Thriving': 1,
		'Blooming': 2,
		'Doing well': 3,
		'Just ok': 4,
		'Coping': 5,
		'Strugglng': 6,
		'Suffering': 7,
		'Depressed': 8,
		'Hopeless': 9,
	}
	
	labels = sorted(list(set(class_names)), key=lambda v: rank_mapping[v])
	dataset_knowns = dataset[dataset['@Ощущаемое.счастье@'] != '@Неизвестно@']
	y_true = np.array([labels.index(xi) for xi in dataset_knowns['@Ощущаемое.счастье@'][:10000].to_numpy()])
	X = dataset_knowns.drop('@Ощущаемое.счастье@', axis=1).values[:10000]
	labels_true = np.array([labels[i] for i in y_true])
	
	import umap
	!pip install umap-learn[plot]
	import umap.plot
	from umap import UMAP
	import time
	from sklearn.cluster import AgglomerativeClustering
	from sklearn.metrics import adjusted_rand_score, silhouette_score, davies_bouldin_score, calinski_harabasz_score
	import matplotlib.pyplot as plt
	
	from sklearn.metrics.pairwise import pairwise_distances
	
	def evaluate_clustering(X, y_true, distance_threshold, metric='euclidean', linkage='ward'):
		clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=distance_threshold, metric=metric, linkage=linkage)
		start = time.process_time()
		y_pred = clustering.fit_predict(X)
		diff = time.process_time() - start
		n_clusters = len(np.unique(y_pred))
		return y_pred, n_clusters, {
			'Time': diff,
			'ARI': adjusted_rand_score(y_true, y_pred),
		}
	
	def find_optimal_clustering(X, y_true, metric='euclidean', linkage='ward', iterations=1000):
		optimal_threshold = None
		best_metrics = None
		y_optimal = None
		n_clusters_optimal = None
		dist = pairwise_distances(X, metric=metric)
		for threshold in np.linspace(np.min(dist), np.max(dist), iterations):
			y_pred, n_clusters, metrics = evaluate_clustering(dist, y_true, threshold, 'precomputed', linkage)
			ari = metrics['ARI']
			if best_metrics is None or ari > best_metrics['ARI']:
				best_metrics = metrics
				y_optimal = y_pred
				optimal_threshold = threshold
				n_clusters_optimal = n_clusters
		
		return optimal_threshold, best_metrics, y_optimal, n_clusters_optimal
	
	import seaborn as sns
	
	def plot_clustering_heatmap(X, y_true):
		distances = ['euclidean', 'manhattan', 'cosine']
		linkages = ['complete', 'average'] # , 'single'
		
		time, ari, n_clusters = np.ndarray((len(distances), len(linkages))), np.ndarray((len(distances), len(linkages))), np.ndarray((len(distances), len(linkages)))
		
		for i, metric in enumerate(distances):
			for j, linkage in enumerate(linkages):
				try:
					_, metrics, _, n = find_optimal_clustering(X, y_true, metric=metric, linkage=linkage, iterations=10)
					time[i, j], ari[i, j], n_clusters[i, j] = metrics['Time'], metrics['ARI'], n
				except Exception as e:
					print(e)
					time[i, j], ari[i, j], n_clusters[i, j] = 0, 0, 0
		
		fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(15, 4))
		sns.heatmap(time, ax=ax1, annot=True, cmap='Reds', xticklabels=linkages, yticklabels=distances)
		sns.heatmap(ari, ax=ax2, annot=True, cmap='Reds', xticklabels=linkages, yticklabels=distances)
		sns.heatmap(n_clusters, ax=ax3, annot=True, cmap='Reds', xticklabels=linkages, yticklabels=distances)
		ax1.set_title('Time')
		ax2.set_title('ARI score')
		ax3.set_title('Clusters count')
		plt.show()
	
	from sklearn.metrics import pairwise_distances
	from collections import Counter
	from mpl_toolkits import mplot3d
	
	def class_purity(y_true, y_pred, cls):
		class_mask = (y_true == cls)
		class_predictions = y_pred[class_mask]
		cluster_counts = Counter(class_predictions).values()
		purity = max(cluster_counts) / len(class_predictions)
		return purity
	
	def plot_clustering(title, X, y_true, y_pred, metric):
		fig, plots = plt.subplots(2, 2, figsize=(12,12))
		fig.suptitle(title)
		plt.prism()
		
		n_clusters = len(np.unique(y_true))
		purities = []
		
		ax = fig.add_subplot(2, 2, 1, projection='3d') if X.shape[1] == 3 else plots[0, 0]
		for i in range(n_clusters):
			digit_indices = (y_true == i)
			purities.append(class_purity(y_true, y_pred, i))
			dims = [X[digit_indices, i] for i in range(X.shape[1])]
			ax.set_title('Original')
			ax.scatter(\*dims, label=f"Class {i}")
			ax.legend()
		
		purities.append(np.average(purities))
		
		avg_dist = np.zeros((n_clusters, n_clusters))
		for i in range(n_clusters):
			for j in range(n_clusters):
				avg_dist[i, j] = pairwise_distances(
					X[y_true == i], X[y_true == j], metric=metric
				).mean()
		avg_dist /= avg_dist.max()
		sns.heatmap(avg_dist, ax=plots[1, 0], annot=True, cmap='Reds', xticklabels=np.arange(n_clusters), yticklabels=np.arange(n_clusters))
		
		inner_distances = [avg_dist[i, i] for i in range(n_clusters)]
		inner_distances.append(np.average(inner_distances))
		sns.heatmap([inner_distances, purities], ax=plots[1, 1], annot=True, cmap='Reds', xticklabels=[\*np.arange(n_clusters), 'avg'], yticklabels=['inner distance', 'purity'])
		
		n_clusters = len(np.unique(y_pred))
		
		ax = fig.add_subplot(2, 2, 2, projection='3d') if X.shape[1] == 3 else plots[0, 1]
		for i in range(n_clusters):
			digit_indices = (y_pred == i)
			dims = [X[digit_indices, i] for i in range(X.shape[1])]
			ax.set_title('Prediction')
			ax.scatter(\*dims, label=f"Cluster {i}")
			ax.legend()
		
		plt.tight_layout()
		plt.legend(loc='best')
		plt.show()
	
	umap_3d_embeddings = UMAP(n_components=3, random_state=7).fit_transform(X)
	umap_3d_embeddings.shape
	
	plot_clustering_heatmap(X, y_true)
	threshold, metrics, y_pred_best, n_clusters = find_optimal_clustering(X, y_true, metric='euclidean', linkage='complete', iterations=100)
	print(metrics, n_clusters)
	plot_clustering('Agglomerative clustering', umap_3d_embeddings, y_true, y_pred_best, 'manhattan')
	
	plot_clustering_heatmap(umap_3d_embeddings, y_true)
	threshold, metrics, y_pred_best, n_clusters = find_optimal_clustering(umap_3d_embeddings, y_true, metric='manhattan', linkage='average', iterations=100)
	print(metrics, n_clusters)
	plot_clustering('3D UMAP embeddings agglomerative clustering', umap_3d_embeddings, y_true, y_pred_best, 'manhattan')
	y_pred_grouped = np.array(itemgetter(\*y_pred_best)(group))
	print(y_pred_grouped.shape)
	y_true_grouped = np.array(itemgetter(\*y_true)(group))
	print(y_true_grouped.shape)
	
	plot_clustering('3D UMAP embeddings agglomerative clustering', umap_3d_embeddings, y_true_grouped, y_pred_grouped, 'manhattan')
	plot_clustering('3D UMAP embeddings agglomerative clustering', umap_3d_embeddings, y_true_grouped, y_pred_best, 'manhattan')
	
	print(f"ARI: {adjusted_rand_score(y_true_grouped, y_pred_best)}")
	print(f"DBI: {davies_bouldin_score(umap_3d_embeddings, y_pred_best)} (original: {davies_bouldin_score(umap_3d_embeddings, y_true_grouped)})")
	print(f"Silhouette: {silhouette_score(umap_3d_embeddings, y_pred_best, random_state=7)} (original: {silhouette_score(umap_3d_embeddings, y_true_grouped, random_state=7)})")
	print(f"Calinski Harabasz: {calinski_harabasz_score(umap_3d_embeddings, y_pred_best)} (original: {calinski_harabasz_score(umap_3d_embeddings, y_true_grouped)})")
	
	dbi_orig = davies_bouldin_score(X, y_true_grouped)
	silhouette_orig = silhouette_score(X, y_true_grouped, random_state=7)
	calinski_harabasz_orig = calinski_harabasz_score(X, y_true_grouped)
	
	dbi_umap = davies_bouldin_score(umap_3d_embeddings, y_true_grouped)
	silhouette_umap = silhouette_score(umap_3d_embeddings, y_true_grouped, random_state=7)
	calinski_harabasz_umap = calinski_harabasz_score(umap_3d_embeddings, y_true_grouped)
	
	dbi_scores = []
	ari_scores = []
	silhouette_scores = []
	calinski_harabasz_scores = []
	
	for k in range(2, 20):
	y_pred = AgglomerativeClustering(n_clusters=k, metric='euclidean', linkage='average').fit_predict(umap_3d_embeddings)
	dbi_scores.append(davies_bouldin_score(umap_3d_embeddings, y_pred))
	ari_scores.append(adjusted_rand_score(y_true_grouped, y_pred))
	silhouette_scores.append(silhouette_score(umap_3d_embeddings, y_pred, random_state=7))
	calinski_harabasz_scores.append(calinski_harabasz_score(umap_3d_embeddings, y_pred))
	
	print(f"Optimal clusters count (DBI):               {np.argmin(dbi_scores)+2}")
	print(f"Optimal clusters count (ARI):               {np.argmax(ari_scores)+2}")
	print(f"Optimal clusters count (Silhouette):        {np.argmax(silhouette_scores)+2}")
	print(f"Optimal clusters count (Calinski Harabasz): {np.argmax(calinski_harabasz_scores)+2}")
	
	fig, plots = plt.subplots(2, 2, figsize=(12,12))
	k_range = np.arange(2, 20)
	
	plots[0, 0].plot(k_range, dbi_scores)
	plots[0, 1].plot(k_range, ari_scores)
	plots[1, 0].plot(k_range, silhouette_scores)
	plots[1, 1].plot(k_range, calinski_harabasz_scores)
	
	plots[0, 0].set_title('DBI')
	plots[0, 1].set_title('ARI')
	plots[1, 0].set_title('Silhouette score')
	plots[1, 1].set_title('Calinski Harabasz score')
	
	right_answer = len(np.unique(y_true))
	plots[0, 0].vlines(right_answer, 0, 1, color='g', label='Right Answer')
	plots[0, 1].vlines(right_answer, 0, 1, color='g', label='Right Answer')
	plots[1, 0].vlines(right_answer, 0, 1, color='g', label='Right Answer')
	plots[1, 1].vlines(right_answer, np.min(calinski_harabasz_scores), np.max(calinski_harabasz_scores), color='g', label='Right Answer')
	
	plots[0, 0].hlines(dbi_orig, 2, 19, color='m', label='Original value from source')
	plots[1, 0].hlines(silhouette_orig, 2, 19, color='m', label='Original value from source')
	plots[1, 1].hlines(calinski_harabasz_orig, 2, 19, color='m', label='Original value from source')
	
	plots[0, 0].hlines(dbi_umap, 2, 19, color='c', label='Original value from UMAP')
	plots[1, 0].hlines(silhouette_umap, 2, 19, color='c', label='Original value from UMAP')
	plots[1, 1].hlines(calinski_harabasz_umap, 2, 19, color='c', label='Original value from UMAP')
	
	plots[0, 0].set_xticks(k_range)
	plots[0, 1].set_xticks(k_range)
	plots[1, 0].set_xticks(k_range)
	plots[1, 1].set_xticks(k_range)
	
	plots[0, 0].legend(loc='best')
	plots[0, 1].legend(loc='best')
	plots[1, 0].legend(loc='best')
	plots[1, 1].legend(loc='best')
	
	plt.show()
	
	from hdbscan import HDBSCAN
	y_pred = HDBSCAN().fit_predict(X)
	plot_clustering('3D UMAP embeddings HDBSCAN clustering', umap_3d_embeddings, y_true_grouped, y_pred, 'euclidean')
	
	def custom_ari(estimator, X):
		labels = estimator.fit_predict(X)
		core_samples_mask = np.zeros_like(labels, dtype=bool)
		core_samples_mask[estimator.labels_ >= 0] = True
		if not np.any(core_samples_mask):
			return 0.0
		true_labels = y_true[core_samples_mask]
		labels = labels[core_samples_mask]
		return adjusted_rand_score(true_labels, labels)
	
	def custom_dbi(estimator, X):
		labels = estimator.fit_predict(X)
		core_samples_mask = np.zeros_like(labels, dtype=bool)
		core_samples_mask[estimator.labels_ >= 0] = True
		if not np.any(core_samples_mask):
			return 0.0
		true_labels = y_true[core_samples_mask]
		labels = labels[core_samples_mask]
		return -davies_bouldin_score(X[core_samples_mask], labels)
	
	def coverage(estimator, X):
		labels = estimator.fit_predict(X)
		core_samples_mask = np.zeros_like(labels, dtype=bool)
		core_samples_mask[estimator.labels_ >= 0] = True
		return np.mean(core_samples_mask)
	
	from sklearn.model_selection import GridSearchCV
	
	def estimate_hdbscan(refit):
		param_grid = {
			'min_cluster_size': np.arange(500, 1500, 100),
			'min_samples': np.arange(1, 30, 3),
			'metric': ['euclidean', 'manhattan', 'cosine']
		}
		hdb = HDBSCAN()
		grid_search = GridSearchCV(hdb, param_grid, scoring={'ARI': custom_ari, 'DBI': custom_dbi, 'coverage': coverage}, refit=refit)
		grid_search.fit(umap_3d_embeddings)
		
		best_params = grid_search.best_params_
		
		y_pred = HDBSCAN(min_cluster_size = grid_search.best_params_['min_cluster_size'],
		min_samples = grid_search.best_params_['min_samples'],
		metric = grid_search.best_params_['metric']).fit_predict(umap_3d_embeddings)
		
		return best_params, y_pred
	
	def visualize_dbscan(best_params, y_pred):
		print(f"Best parameters: {best_params}")
		core_samples_mask = np.zeros_like(y_pred, dtype=bool)
		core_samples_mask[y_pred >= 0] = True
		cov = np.mean(core_samples_mask)
		
		print(f"ARI: {adjusted_rand_score(y_true_grouped, y_pred)}")
		print(f"DBI: {davies_bouldin_score(umap_3d_embeddings, y_pred)}")
		print(f"ARI (covered): {adjusted_rand_score(y_true_grouped[core_samples_mask], y_pred[core_samples_mask])}")
		print(f"DBI (covered): {davies_bouldin_score(umap_3d_embeddings[core_samples_mask], y_pred[core_samples_mask])}")
		print(f"Coverage: {cov}")
		print(f"N clusters: {len(np.unique(y_pred))}")
		plot_clustering('3D UMAP embeddings HDBSCAN clustering', umap_3d_embeddings, y_true_grouped, y_pred, 'euclidean')
	
	best_params, y_pred = estimate_hdbscan('ARI')
	visualize_dbscan(best_params, y_pred)
\end{lstlisting}

\clearpage
