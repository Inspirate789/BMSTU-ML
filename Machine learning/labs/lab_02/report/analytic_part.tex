\chapter{Аналитическая часть}



\section{Модель полиномиальной регрессии}

Полиномиальная регрессия является одним из наиболее широко используемых алгоритмов машинного обучения, нацеленных на аппроксимацию нелинейной зависимости между переменными. Основная 
идея состоит в том, чтобы найти полиномиальное выражение для целевой переменной на основе набора входных переменных.

Пусть $\mathbf{x}$ является набором входных переменных и $y$ — целевой функцией, которую мы хотим предсказать. Цель полиномиальной регрессии — найти коэффициенты $w = (w_0, w_1, ..., 
w_n)$ в таких, что

$$y \approx w^T\phi(\mathbf{x})$$где $\phi$ — функция, которая преобразует исходные данные в набор признаков.



\section{Регуляризация}

Регуляризация (англ. regularization) в статистике и машинном обучении — метод добавления некоторых дополнительных ограничений к условию с целью предотвратить переобучение. Чаще всего эта информация имеет вид штрафа за сложность модели.

Если выбрана излишне сложная модель при недостаточном объеме данных, то в итоге может быть получена модель, которая хорошо описывает обучающую выборку, но не обобщается на тестовую. Переобучение в большинстве случаев проявляется в том, что итоговые модели имеют слишком большие значения параметров. Одним из способов борьбы с негативным эффектом излишнего подстраивания под данные — использование регуляризации, т.е. добавление некоторого штрафа за большие значения коэффициентов у линейной модели. Тем самым запрещаются слишком <<резкие>> изгибы, и предотвращается переобучение.

Наиболее часто используемые виды регуляризации — $L_1$ и $L_2$, а также их линейная комбинация — эластичная сеть. 

В представленных ниже формулах для эмпирического риска $Q$ приняты следующие обозначения: $L$ --- функция потерь, $\beta$ --- вектор параметров $g(x_i, \beta)$ из модели алгоритма, $\lambda$ – неотрицательный гиперпараметр (коэффициент регуляризации).

Если в качестве функционал качества используется сумма квадратов остатков (Residual Sum of Squares - RSS), тогда изначально:
\begin{equation}
L(y_i, g(x_i, \beta)) = (g(x_i, \beta) - y_i)^2
\end{equation}
\begin{equation}
RSS = Q(\beta, X') = \sum_{i=1}^{l} L(y_i, g(x_i, \beta)) = \sum_{i=1}^{l} (g(x_i, \beta) - y_i)^2
\end{equation}

$ L_2 $-регуляризация (ridge regularization) или регуляризация Тихонова (Tikhonov regularization):
\begin{equation}
Q(\beta, X') = \sum_{i=1}^{l} L(y_i, g(x_i, \beta)) + \lambda \sum_{j=1}^{n} \beta_j^2
\end{equation}

Минимизация регуляризованного соотвествующим образом эмпирического риска приводит к выбору такого вектора параметров $ \beta $, которое не слишком сильно отклоняется от нуля. В линейных классификаторах это позволяет избежать проблем мультиколлинеарности и переобучения. 

$ L_1 $-регуляризация (lasso regularization) или регуляризация через манхэттенское расстояние:
\begin{equation}
	Q(\beta, X') = \sum_{i=1}^{l} L(y_i, g(x_i, \beta)) + \lambda \sum_{j=1}^{n} |\beta_j|
\end{equation}

Данный вид регуляризации также позволяет ограничить значения вектора $ \beta $. Однако, к тому же обладает интересными и полезными свойствами — обнуляет значения некоторых параметров, что в случае с линейными моделями приводит к отбору признаков.


\clearpage
