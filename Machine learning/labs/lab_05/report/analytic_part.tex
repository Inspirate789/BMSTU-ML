\chapter{Аналитическая часть}



\section{Теорема Байеса}

Теорема Байеса (или формула Байеса) — одна из основных теорем
элементарной теории вероятностей, которая позволяет определить
вероятность какого-либо события при условии, что произошло другое
статистически взаимозависимое с ним событие. Другими словами, по
формуле Байеса (\ref{eq:bayes}) можно более точно пересчитать вероятность, взяв в расчёт
как ранее известную информацию, так и данные новых наблюдений.
\begin{equation}
\label{eq:bayes}
	P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)},
\end{equation}
где
\begin{itemize}[label*=---]
	\item $P(A)$ — априорная вероятность гипотезы $A$
	(prior probability);
	\item $P(A|B)$ — вероятность гипотезы $A$ при
	наступлении события $B$ (апостериорная
	вероятность --- posterior probability);
	\item $P(B|A)$ — вероятность наступления
	события $B$ при истинности гипотезы $A$
	(правдоподобие --- likelihood);
	\item $P(B)$ — полная вероятность наступления
	события $B$ (вероятность данных evidence).
\end{itemize}

\section{Классификация}

Классификация (classification) — это задача присвоения меток класса
(class label) наблюдениям (Observation) объектам из предметной области.
Множество допустимых меток класса конечно. В свою очередь класс —
это множество всех объектов с данным значением метки. Требуется
построить алгоритм, способный классифицировать (присвоить метку)
произвольный объект из исходного множества. Классификация, как
правило, на этапе настройки использует обучение с учителем.

\section{Наивный байессовский классификатор}

Широкий класс алгоритмов классификации, основанный на принципе максимума
апостериорной вероятности. Для классифицируемого объекта вычисляются функции
правдоподобия каждого из классов, по ним вычисляются апостериорные вероятности
классов. Байесовский классификатор использует оценку апостериорного максимума
(Maximum a posteriori estimation) для определения наиболее вероятного класса.
Объект относится к тому классу, для которого апостериорная вероятность
максимальна.

Байесовский подход к классификации основан на теореме, утверждающей, что
если плотности распределения каждого из классов известны, то искомый алгоритм
можно выписать в явном аналитическом виде. Более того, этот алгоритм оптимален,
то есть обладает минимальной вероятностью ошибок.

На практике плотности распределения классов не известны. Их приходится
оценивать (восстанавливать) по обучающей выборке. В результате байесовский
алгоритм перестаёт быть оптимальным, так как восстановить плотность по выборке
можно только с некоторой погрешностью. Чем короче выборка, тем выше шансы
подогнать распределение под конкретные данные и столкнуться с эффектом
переобучения.

Вероятностная модель для классификатора — это условная модель $P(C|F_1, \dots, F_n)$
над зависимой переменной класса $C$ с малым количеством результатов или
классов, зависимая от нескольких переменных $F_1, \dots, F_n$.

Используя теорему Байеса, запишем:
\begin{equation}
	P(C|F_1, \dots, F_n) = \frac{P(C)P(F_1, \dots, F_n|C)}{P(F_1, \dots, F_n)}.
\end{equation}
На практике интересен лишь числитель этой дроби, так как знаменатель не
зависит от $C$ и значения свойств $F_i$ даны, так что знаменатель — константа.
Числитель эквивалентен совместной вероятности модели $P(C, F_1, \dots, F_n))$,
которая может быть переписана, используя повторные приложения
определений условной вероятности:

\begin{equation}
\begin{split}
	P(C, F_1, \dots, F_n) = P(C)P(F_1, \dots, F_n|C) 
	=\\= P(C)P(F_1|C)P(F_2, \dots, F_n|C, F_1) 
	=\\= P(C)P(F_1|C)P(F_2|C,F_1)P(F_3, \dots, F_n|C, F_1, F_2) 
	=\\= P(C)P(F_1|C)P(F_2|C,F_1) \dots P(F_n, \dots, F_n|C, F_1, F_2, F_3, \dots, F_{n-1}).
\end{split}
\end{equation}



\clearpage
