\chapter{Технологическая часть}

\section{Средства реализации}

В качестве языка программирования для реализации алгоритмов был выбран язык программирования Python ввиду наличия библиотек для обучения регрессионных моделей, таких как sklearn и numpy.

\section{Реализация алгоритмов}

На листинге \ref{lst:1} представлена реализация алгоритма обучения классификаторов респондентов, принимавших участие в социологическом исследовании.

\begin{lstlisting}[label=lst:1,caption=Кластеризация с использованием генетического алгоритма]
	import numpy
	import sklearn.datasets
	!pip install pygad
	import pygad
	from sklearn.metrics import pairwise_distances
	from collections import Counter
	import seaborn as sns
	import matplotlib.pyplot as plt
	from mpl_toolkits import mplot3d
	# %matplotlib inline
	
	import umap
	!pip install umap-learn[plot]
	import umap.plot
	from umap import UMAP
	from sklearn.metrics import adjusted_rand_score, silhouette_score, davies_bouldin_score, calinski_harabasz_score
	
	images, target = sklearn.datasets.load_digits(return_X_y=True, as_frame=True)
	print(images.shape)
	
	import numpy as np
	
	X, y_true = images, target.astype(int)
	X = X.astype(np.float32) / 255.0
	
	umap_3d_embeddings = UMAP(n_components=3, random_state=7).fit_transform(X)
	
	def euclidean_distance(X, Y):
		return numpy.sqrt(numpy.sum(numpy.power(X - Y, 2), axis=1))
	
	def cluster_data(solution, solution_idx):
		global num_clusters, feature_vector_length, data
		cluster_centers = []
		all_clusters_dists = []
		clusters = []
		clusters_sum_dist = []
		
		for clust_idx in range(num_clusters):
			cluster_centers.append(solution[feature_vector_length*clust_idx:feature_vector_length*(clust_idx+1)])
			cluster_center_dists = euclidean_distance(data, cluster_centers[clust_idx])
			all_clusters_dists.append(numpy.array(cluster_center_dists))
		
		cluster_centers = numpy.array(cluster_centers)
		all_clusters_dists = numpy.array(all_clusters_dists)
		
		cluster_indices = numpy.argmin(all_clusters_dists, axis=0)
		for clust_idx in range(num_clusters):
			clusters.append(numpy.where(cluster_indices == clust_idx)[0])
			if len(clusters[clust_idx]) == 0:
				clusters_sum_dist.append(0)
			else:
				clusters_sum_dist.append(numpy.sum(all_clusters_dists[clust_idx, clusters[clust_idx]]))
		
		clusters_sum_dist = numpy.array(clusters_sum_dist)
		return cluster_centers, all_clusters_dists, cluster_indices, clusters, clusters_sum_dist
	
	def fitness_func(ga_instance, solution, solution_idx):
		_, _, cluster_indices, _, _ = cluster_data(solution, solution_idx)
		return adjusted_rand_score(y_true, cluster_indices)
	
	data = umap_3d_embeddings
	num_clusters = 10
	feature_vector_length = data.shape[1]
	num_genes = num_clusters * feature_vector_length
	
	ga_instance = pygad.GA(num_generations=500,
												sol_per_pop=60,
												init_range_low=-10,
												init_range_high=20,
												num_parents_mating=50,
												keep_parents=40,
												num_genes=num_genes,
												fitness_func=fitness_func)
	
	ga_instance.run()
	
	best_solution, best_solution_fitness, best_solution_idx = ga_instance.best_solution()
	print("Best solution is {bs}".format(bs=best_solution))
	print("Fitness of the best solution is {bsf}".format(bsf=best_solution_fitness))
	print("Best solution found after {gen} generations".format(gen=ga_instance.best_solution_generation))
	
	cluster_centers, all_clusters_dists, cluster_indices, clusters, clusters_sum_dist = cluster_data(best_solution, best_solution_idx)
	print(f'Clusters: {np.unique(cluster_indices)}')
	print(f'ARI: {adjusted_rand_score(y_true, cluster_indices)}')
	print(f'DBI: {davies_bouldin_score(data, cluster_indices)}')
	print(f'Silhouette: {silhouette_score(data, cluster_indices, random_state=7)}')
	print(f'Calinski Harabasz: {calinski_harabasz_score(data, cluster_indices)}')
	
	def class_purity(y_true, y_pred, cls):
		class_mask = (y_true == cls)
		class_predictions = y_pred[class_mask]
		cluster_counts = Counter(class_predictions)
		purity = max(cluster_counts.values()) / len(class_predictions)
		return purity
	
	def plot_clustering(title, X, y_true, y_pred, metric):
		fig, plots = plt.subplots(2, 2, figsize=(12,12))
		fig.suptitle(title)
		plt.prism()
		
		n_clusters = len(np.unique(y_true))
		purities = []
		
		ax = fig.add_subplot(2, 2, 1, projection='3d') if X.shape[1] == 3 else plots[0, 0]
		for i in range(n_clusters):
			digit_indices = (y_true == i)
			purities.append(class_purity(y_true, y_pred, i))
			dims = [X[digit_indices, i] for i in range(X.shape[1])]
			ax.set_title('Original')
			ax.scatter(\*dims, label=f"Digit {i}")
			ax.legend()
		
		purities.append(np.average(purities))
		
		avg_dist = np.zeros((n_clusters, n_clusters))
		for i in range(n_clusters):
			for j in range(n_clusters):
				avg_dist[i, j] = pairwise_distances(
					X[y_true == i], X[y_true == j], metric=metric
				).mean()
		avg_dist /= avg_dist.max()
		sns.heatmap(avg_dist, ax=plots[1, 0], annot=True, cmap='Reds', xticklabels=np.arange(n_clusters), yticklabels=np.arange(n_clusters))
		
		inner_distances = [avg_dist[i, i] for i in range(n_clusters)]
		inner_distances.append(np.average(inner_distances))
		sns.heatmap([inner_distances, purities], ax=plots[1, 1], annot=True, cmap='Reds', xticklabels=[*np.arange(n_clusters), 'avg'], yticklabels=['inner distance', 'purity'])
		
		n_clusters = len(np.unique(y_pred))
		
		ax = fig.add_subplot(2, 2, 2, projection='3d') if X.shape[1] == 3 else plots[0, 1]
		for i in range(n_clusters):
			digit_indices = (y_pred == i)
			dims = [X[digit_indices, i] for i in range(X.shape[1])]
			ax.set_title('Prediction')
			ax.scatter(\*dims, label=f"Cluster {i}")
			ax.legend()
		
		plt.tight_layout()
		plt.legend(loc='best')
		plt.show()
	
	plot_clustering('Prediction', data, y_true, cluster_indices, 'euclidean')
	ga_instance.plot_fitness()
\end{lstlisting}

\clearpage
